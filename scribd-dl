#!/bin/bash
type seq || { echo >&2 "Seq command is required. Aborting."; exit 1; }
type curl || { echo >&2 "Curl is required. Aborting."; exit 1; }
type gunzip || { echo >&2 "Gzip is required. Aborting."; exit 1; }
type convert || { echo >&2 "Image Magick is required. Aborting."; exit 1; }
type ex || { echo >&2 "Ex editor is required. Aborting."; exit 1; }
type awk || { echo >&2 "Awk is required. Aborting."; exit 1; }
type sed || { echo >&2 "Sed is required. Aborting."; exit 1; }
type tee || { echo >&2 "Tee is required. Aborting."; exit 1; }
: ${1?"Usage: $0 (url)"}

# Few helpful functions.
next() { echo $1; }
rest() { shift; echo $*; }
sprintf() { local stdin; read -d '' -u 0 stdin; printf "$@" "$stdin"; }
clean-up() { cd -; printf "Cleaning up... "; rm -r "$TMPDIR" && echo "done."; }

TMPDIR=".${0##*/}-$$" && mkdir -v "$TMPDIR"
TMPDIR=".scribd-dl-34373"
cd "$TMPDIR"
#trap 'echo; clean-up; exit 1' SIGINT SIGTERM

# Download the main page.
PAGE="page.html"
html=$(curl -LA Mozilla "$1" | tee $PAGE) || exit 1
html=$(cat $PAGE)
cp -vf $PAGE $PAGE.org

# Determine PDF file name based on the title.
filename="${html#*<title>}"
filename="${filename%%</title>*}"
filename="${filename//\//-}"

# Get prefix id.
regexPrefix="docManager.assetPrefix = \""
prefix="${html#*$regexPrefix}"
prefix="${prefix%%\"*}"

## Parse html page.
ex -V1 $PAGE <<-EOF
  " Correcting missing protocol, see: https://github.com/wkhtmltopdf/wkhtmltopdf/issues/2359 "
  %s,'//,'http://,ge
  %s,"//,"http://,ge
  " Correcting relative paths, see: https://github.com/wkhtmltopdf/wkhtmltopdf/issues/2359 "
  %s,[^,]\zs'/\ze[^>],'http://www.scribd.com/,ge
  %s,[^,]\zs"/\ze[^>],"http://www.scribd.com/,ge
  " Remove the margin on the left of the main block. "
  %s/id="doc_container"/id="doc_container" style="min-width:0px;margin-left : 0px;"/g
  %s/<div class="outer_page/<div style="margin: 0px;" class="outer_page/g
  " Remove useless html elements. "
  /<div.*id="global_header"/norm nvatd
  /<div class="header_spacer"/norm nvatd
  /<div.*id="doc_info"/norm nvatd
  /<div.*class="toolbar_spacer"/norm nvatd
  /<div.*between_page_ads_1/norm nvatd
  /id="leaderboard_ad_main">/norm nvatd
  /class="page_missing_explanation/norm nvatd
  /<div id="between_page_ads/norm nvatd
  /<div class="b_..">/norm nvatd
  /<div class="buy_doc_bar/norm nvatd
  /<div class="shadow_overlay">/norm nvatd
  /grab_blur_promo_here/norm nvatd
  /missing_page_buy_button/norm nvatd
  wq " Update changes and quit.
EOF

function remove_node {
  # $1 is the node regexp string
  # $2 is the file
  node_regex=$1
  filename=$2
  commande="{if(!i && /${node_regex}/){i=1}else{if(i){if(/<div/){i++} if(/<\/div>/){i--}}else{if(!i){print \$0}} }}"
  output=$(awk "$commande" "$filename") && echo "$output" > "$filename"
}

function remove_n_node {
  # $1 is the node regexp string
  # $2 is the file
  node_regex=$1
  filename=$2
  n=$3
  commande="BEGIN {l=${n}} {if( !i && l>0 && /${node_regex}/ ){i=1;l--}else{if(i){if(/<div/){i++} if(/<\/div>/){i--}}else{if(!i){print \$0}} }}"
  output=$(awk "$commande" "$filename") && echo "$output" > "$filename"
}

function keep_n_node {
  # $1 is the node regexp string
  # $2 is the file
  node_regex=$1
  filename=$2
  n=$3
  commande="BEGIN {l=${n}} {if(l > 0 && /${node_regex}/ ){l--;print \$0}else{if(!i && /${node_regex}/ ){i=1;l--}else{if(i){if(/<div/){i++} if(/<\/div>/){i--}}else{if(!i){print \$0}} }}}"
  output=$(awk "$commande" "$filename") && echo "$output" > "$filename"
}

function remove_errors {
  filename=$1
  output=$(awk '/</{i++}i' "$filename")
  echo $output
  exit
  echo "$output" > "$filename"
}

# We remove all html elements which are useless (such as menus)
remove_errors $PAGE

# Get content page URLs.
urls=$(echo $html | grep -o '"[^" ]\+.jsonp"' | tr -d '"')
totalPages=$(c() { echo $#; }; c $urls)
echo "Found $totalPages pages. Downloading pages..."

# Download pages.
for i in `seq -w 1 $totalPages`; do
  url=$(next $urls)
  filename="page-$i-$(basename $url)"
  wget -O "$TMPDIR/$filename.gz" -c --user-agent=Mozilla $url || exit 1
  urls=$(rest $urls) # Remove first URL from the list.
done

# Uncompress gzip files.
ls $TMPDIR/*.gz && gunzip -fv $TMPDIR/*.gz

# Download images.
images=$(grep -oh '[^"]*images/[^"]*' $TMPDIR/*.jsonp | sort | uniq | tr -d \\)
mkdir -v $TMPDIR/images
wget -P "$TMPDIR/images" -c --user-agent=Mozilla $images

# Parse content pages.
grep -oh '".*"' $TMPDIR/*.jsonp |
  while read -r line; do
    line="${line%\"}" # Remove first double-quote.
    line="${line#\"}" # Remove last double-quote.
    echo -e $line | sprintf
  done > "$TMPDIR/$filename.html"
ex +'%s/http:.*\zeimages[^"]*//g' +'%s/orig=/src=/g' "$TMPDIR/$filename.html"

if ! convert "$WORKINGDIR/$file.jpg" "$WORKINGDIR/$file.pdf"; then
  printf '%s\n' "error." "Could not convert \"$WORKINGDIR/$file.jpg\" to PDF."
  clean-up
  exit 1
fi

printf 'Combining all PDF files to one file... '
pdftk "$WORKINGDIR/"*".pdf" cat output "$WORKINGDIR/$filename.pdf" || exit 1
mv -v "$WORKINGDIR/$filename.pdf" "$filename.pdf"

echo "$(($(stat --printf '%s' "$TMPDIR/$filename.pdf") / 1024)) KB."
echo "PDF file saved as '$filename.pdf'."
clean-up
